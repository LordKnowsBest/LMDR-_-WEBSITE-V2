/**
 * Observability Service - System logging and tracing for Super Admin
 * Provides comprehensive visibility into system operations
 * 
 * This service uses unified dataAccess for dual-source support.
 *
 * @module backend/observabilityService
 */

import * as dataAccess from 'backend/dataAccess';
import { currentMember } from 'wix-members-backend';

// ============================================
// COLLECTION KEYS FOR DUAL-SOURCE ROUTING
// ============================================

const COLLECTION_KEYS = {
    logs: 'systemLogs',
    traces: 'systemTraces',
    errors: 'systemErrors',
    metrics: 'systemMetrics',
    alerts: 'systemAlerts',
    anomalyAlerts: 'anomalyAlerts',
    anomalyRules: 'anomalyRules',
    baselineMetrics: 'baselineMetrics',
    aiUsageLog: 'aiUsageLog'
};

const AGENT_SOURCES = [
    'agent-orchestrator',
    'agent-run-ledger',
    'agent-outcome',
    'b2b-research-agent',
    'self-healing',
    'ai-router'
];

// ============================================
// CONFIGURATION
// ============================================

const CONFIG = {
    // Log levels
    levels: {
        DEBUG: 0,
        INFO: 1,
        WARN: 2,
        ERROR: 3,
        CRITICAL: 4
    },
    // Log sources/services
    sources: [
        'ai-enrichment',
        'ai-router',
        'carrier-matching',
        'driver-matching',
        'social-scanner',
        'ocr-service',
        'b2b-research-agent',
        'agent-orchestrator',
        'agent-run-ledger',
        'agent-outcome',
        'self-healing',
        'scheduler',
        'auth',
        'api',
        'database',
        'external-api',
        'system'
    ],
    logSampling: {
        minPersistLevel: 'DEBUG',
        maxDetailsLength: 12000
    },
    metricsSampling: {
        minPersistMs: 100,
        forcePersistSources: ['database', 'external-api', 'ai-router', 'agent-orchestrator']
    }
};

// ============================================
// AUTHORIZATION
// ============================================

async function isSuperAdmin() {
    try {
        const member = await currentMember.getMember({ fieldsets: ['FULL'] });
        if (!member) return false;
        const role = member.contactDetails?.customFields?.role || '';
        return role.toLowerCase() === 'super_admin';
    } catch (error) {
        return false;
    }
}

async function requireSuperAdmin() {
    if (!(await isSuperAdmin())) {
        throw new Error('Unauthorized: Super Admin access required');
    }
}

// ============================================
// AIRTABLE WRITE THROTTLE
// ============================================
const PERSIST_THRESHOLD = CONFIG.levels[CONFIG.logSampling.minPersistLevel] || 0;
let __logFailCount = 0;
const LOG_FAIL_THRESHOLD = 3;
let __metricWriteGuard = 0;
const ACTIVE_TRACES = new Map();

// ============================================
// LOGGING FUNCTIONS
// ============================================

/**
 * Log an event to the system
 */
export async function log(logEntry) {
    const level = String(logEntry.level || 'INFO').toUpperCase();
    const entry = {
        timestamp: new Date().toISOString(),
        level,
        level_num: CONFIG.levels[level] ?? CONFIG.levels.INFO,
        source: logEntry.source || 'system',
        message: String(logEntry.message || ''),
        details: sanitizeForStorage(logEntry.details || {}),
        trace_id: logEntry.traceId || generateTraceId(),
        span_id: logEntry.spanId || generateSpanId(),
        parent_span_id: logEntry.parentSpanId || null,
        user_id: logEntry.userId || null,
        session_id: logEntry.sessionId || null,
        duration: logEntry.duration || null,
        tags: Array.isArray(logEntry.tags) ? logEntry.tags.join(', ') : (logEntry.tags || '')
    };

    if (entry.level_num < PERSIST_THRESHOLD || __logFailCount >= LOG_FAIL_THRESHOLD) {
        return entry;
    }

    try {
        const detailsJson = safeJson(entry.details);
        await dataAccess.insertRecord(COLLECTION_KEYS.logs, {
            log_id: entry.span_id || generateSpanId(),
            level: entry.level.toLowerCase(),
            service: entry.source,
            message: entry.message,
            log_date: entry.timestamp,
            legacy_wix_id: null
        }, { suppressAuth: true, traceId: entry.trace_id });

        if (entry.level_num >= CONFIG.levels.ERROR) {
            const errorRecord = {
                timestamp: entry.timestamp,
                level: entry.level,
                level_num: entry.level_num,
                source: entry.source,
                message: entry.message,
                details: detailsJson,
                trace_id: entry.trace_id,
                span_id: entry.span_id,
                parent_span_id: entry.parent_span_id,
                user_id: entry.user_id,
                session_id: entry.session_id,
                duration: entry.duration,
                tags: entry.tags
            };
            await dataAccess.insertRecord(COLLECTION_KEYS.errors, errorRecord, { suppressAuth: true, traceId: entry.trace_id });
        }
        __logFailCount = 0;
    } catch (error) {
        __logFailCount++;
        if (__logFailCount === LOG_FAIL_THRESHOLD) {
            console.warn('[Observability] Circuit breaker tripped - logging disabled');
        }
    }

    return entry;
}
/**
 * Log an AI operation
 */
export async function logAIOperation(aiLog) {
    const latencyMs = Number(aiLog.latencyMs || 0);
    const tokensUsed = Number(aiLog.tokensUsed || aiLog.tokens || 0);
    const inputTokens = Number(aiLog.inputTokens || aiLog.promptTokens || 0);
    const outputTokens = Number(aiLog.outputTokens || aiLog.completionTokens || 0);

    await dataAccess.insertRecord(COLLECTION_KEYS.aiUsageLog, {
        functionId: aiLog.functionId || aiLog.operation || 'unknown',
        provider: aiLog.provider || 'unknown',
        model: aiLog.model || aiLog.modelId || 'unknown',
        latencyMs,
        tokensUsed,
        inputTokens,
        outputTokens,
        error: aiLog.error ? String(aiLog.error) : null,
        usedFallback: !!aiLog.usedFallback,
        timestamp: aiLog.timestamp || new Date().toISOString()
    }, { suppressAuth: true, traceId: aiLog.traceId }).catch(() => {});

    return log({
        level: aiLog.error ? 'ERROR' : 'INFO',
        source: aiLog.source || 'ai-router',
        message: `AI ${aiLog.operation}: ${aiLog.functionId}`,
        details: aiLog,
        traceId: aiLog.traceId,
        spanId: aiLog.spanId,
        duration: aiLog.latencyMs,
        tags: ['ai', aiLog.provider, aiLog.functionId]
    });
}

/**
 * Log an external API call
 */
export async function logExternalAPI(apiLog) {
    return log({
        level: apiLog.statusCode >= 400 ? 'ERROR' : 'INFO',
        source: 'external-api',
        message: `${apiLog.method} ${apiLog.endpoint} - ${apiLog.statusCode}`,
        details: apiLog,
        traceId: apiLog.traceId,
        spanId: apiLog.spanId,
        duration: apiLog.latencyMs,
        tags: ['external-api', apiLog.service]
    });
}

/**
 * Log a database operation
 */
export async function logDatabase(dbLog) {
    const latencyMs = Number(dbLog.latencyMs || 0);
    const source = dbLog.source || 'database';
    const forcePersist = CONFIG.metricsSampling.forcePersistSources.includes(source);
    if (!forcePersist && latencyMs < CONFIG.metricsSampling.minPersistMs) {
        return { timestamp: new Date().toISOString(), level: 'DEBUG', source };
    }

    if (__metricWriteGuard > 0) {
        return { timestamp: new Date().toISOString(), level: 'DEBUG', source };
    }

    __metricWriteGuard++;
    try {
        await dataAccess.insertRecord(COLLECTION_KEYS.metrics, {
            timestamp: new Date().toISOString(),
            metric_name: `db.${dbLog.operation || 'unknown'}.latency_ms`,
            value: latencyMs,
            source,
            tags: toTagString([
                'database',
                dbLog.collection ? `collection:${dbLog.collection}` : null,
                dbLog.success === false ? 'status:error' : 'status:ok'
            ]),
            metadata: safeJson({
                operation: dbLog.operation || 'unknown',
                collection: dbLog.collection || 'unknown',
                affectedRows: Number(dbLog.affectedRows || 0),
                success: dbLog.success !== false
            })
        }, { suppressAuth: true, traceId: dbLog.traceId });
    } catch (error) {
        // Best effort only.
    } finally {
        __metricWriteGuard--;
    }

    return { timestamp: new Date().toISOString(), level: 'DEBUG', source };
}

// ============================================
// TRACING
// ============================================

/**
 * Start a new trace
 */
export async function startTrace(name, metadata = {}) {
    const traceId = generateTraceId();
    const startTime = new Date();
    const tags = Array.isArray(metadata.tags) ? metadata.tags : [];
    const state = {
        traceId,
        name,
        startTime,
        metadata: sanitizeForStorage(metadata),
        tags,
        spans: []
    };
    ACTIVE_TRACES.set(traceId, state);

    await dataAccess.insertRecord(COLLECTION_KEYS.traces, {
        trace_id: traceId,
        name: String(name || 'operation'),
        start_time: startTime.toISOString(),
        end_time: null,
        status: 'in_progress',
        spans: safeJson([]),
        metadata: safeJson(state.metadata),
        tags: tags.join(', '),
        duration: null,
        summary: ''
    }, { suppressAuth: true, traceId }).catch(() => {});

    return {
        traceId,
        startSpan: (spanName, parentSpanId = null) => startSpan(traceId, spanName, parentSpanId)
    };
}

/**
 * Start a span within a trace
 */
export function startSpan(traceId, name, parentSpanId = null) {
    const spanId = generateSpanId();
    const startTime = Date.now();
    const traceState = ACTIVE_TRACES.get(traceId);
    if (traceState) {
        traceState.spans.push({
            spanId,
            parentSpanId,
            name: String(name || 'span'),
            startTime: new Date(startTime).toISOString(),
            endTime: null,
            status: 'running',
            duration: null,
            metadata: {}
        });
    }

    return {
        traceId,
        spanId,
        parentSpanId,
        name,
        startTime,
        end: async (status = 'success', metadata = {}) => {
            const duration = Date.now() - startTime;
            const trace = ACTIVE_TRACES.get(traceId);
            if (trace) {
                const idx = trace.spans.findIndex((span) => span.spanId === spanId);
                if (idx >= 0) {
                    trace.spans[idx] = {
                        ...trace.spans[idx],
                        endTime: new Date().toISOString(),
                        status,
                        duration,
                        metadata: sanitizeForStorage(metadata || {})
                    };
                }
            }
            await log({
                level: status === 'error' ? 'ERROR' : 'DEBUG',
                source: 'trace',
                message: `Span: ${name}`,
                traceId,
                spanId,
                parentSpanId,
                duration,
                details: { status, ...metadata }
            });
            return { duration, status };
        }
    };
}

/**
 * End a trace
 */
export async function endTrace(traceId, status = 'completed', summary = {}) {
    const endTime = new Date();
    const trace = ACTIVE_TRACES.get(traceId);
    const startTime = trace?.startTime ? new Date(trace.startTime) : endTime;
    const duration = Math.max(0, endTime.getTime() - startTime.getTime());
    const traceRecord = await dataAccess.queryRecords(COLLECTION_KEYS.traces, {
        filters: { trace_id: traceId },
        limit: 1,
        suppressAuth: true,
        traceId
    }).catch(() => ({ items: [] }));
    const existing = traceRecord?.items?.[0];

    if (existing?._id) {
        await dataAccess.updateRecord(COLLECTION_KEYS.traces, {
            _id: existing._id,
            trace_id: traceId,
            name: trace?.name || existing.name || 'operation',
            start_time: trace?.startTime ? new Date(trace.startTime).toISOString() : (existing.start_time || startTime.toISOString()),
            end_time: endTime.toISOString(),
            status: String(status || 'completed'),
            spans: safeJson(trace?.spans || []),
            metadata: safeJson(trace?.metadata || {}),
            tags: toTagString(trace?.tags || []),
            duration,
            summary: safeJson(sanitizeForStorage(summary || {}))
        }, { suppressAuth: true, traceId }).catch(() => {});
    }

    ACTIVE_TRACES.delete(traceId);
    return { traceId, status, duration };
}

// ============================================
// QUERY FUNCTIONS (Super Admin)
// ============================================

export async function getLogs(options = {}) {
    await requireSuperAdmin();

    try {
        const filters = {};
        if (options.level) filters.level = String(options.level).toLowerCase();
        if (options.source) filters.service = options.source;
        
        const result = await dataAccess.queryRecords(COLLECTION_KEYS.logs, {
            filters,
            sort: [{ field: 'log_date', direction: 'desc' }],
            limit: options.limit || 50,
            skip: options.skip || 0,
            suppressAuth: true
        });

        const items = (result.items || []).map(normalizeLogRecord);
        return {
            items,
            totalCount: result.totalCount || items.length,
            hasMore: items.length === (options.limit || 50),
            page: Math.floor((options.skip || 0) / (options.limit || 50)) + 1,
            pageSize: options.limit || 50
        };
    } catch (error) {
        console.error('[Observability] Error fetching logs:', error.message);
        throw new Error('Failed to fetch logs');
    }
}

export async function getTrace(traceId) {
    await requireSuperAdmin();

    try {
        const traceRes = await dataAccess.queryRecords(COLLECTION_KEYS.traces, {
            filters: { trace_id: traceId },
            limit: 1,
            suppressAuth: true
        });

        if (!traceRes.success || traceRes.items.length === 0) {
            throw new Error('Trace not found');
        }

        const errorLogs = await dataAccess.queryRecords(COLLECTION_KEYS.errors, {
            filters: { trace_id: traceId },
            sort: [{ field: 'timestamp', direction: 'asc' }],
            limit: 100,
            suppressAuth: true
        });

        const trace = normalizeTraceRecord(traceRes.items[0]);
        const logs = (errorLogs.items || []).map(normalizeErrorRecord);
        const timeline = buildTraceTimeline(trace, logs);
        return { trace, logs, timeline };
    } catch (error) {
        console.error('[Observability] Error fetching trace:', error.message);
        throw new Error('Failed to fetch trace');
    }
}

export async function getErrors(options = {}) {
    await requireSuperAdmin();
    const period = options.period || 'day';
    const now = new Date();
    const start = new Date(now);
    if (period === 'hour') start.setHours(now.getHours() - 1);
    else if (period === 'week') start.setDate(now.getDate() - 7);
    else if (period === 'month') start.setDate(now.getDate() - 30);
    else start.setDate(now.getDate() - 1);

    const result = await dataAccess.queryRecords(COLLECTION_KEYS.errors, {
        filters: { timestamp: { gte: start } },
        sort: [{ field: 'timestamp', direction: 'desc' }],
        limit: options.limit || 200,
        suppressAuth: true
    });

    const items = (result.items || []).map(normalizeErrorRecord);
    const bySource = items.reduce((acc, item) => {
        const key = item.source || 'unknown';
        acc[key] = (acc[key] || 0) + 1;
        return acc;
    }, {});

    const byHourMap = {};
    for (const item of items) {
        const d = new Date(item.timestamp || now);
        const key = `${d.getFullYear()}-${String(d.getMonth() + 1).padStart(2, '0')}-${String(d.getDate()).padStart(2, '0')}T${String(d.getHours()).padStart(2, '0')}`;
        byHourMap[key] = (byHourMap[key] || 0) + 1;
    }
    const byHour = Object.entries(byHourMap).sort((a, b) => a[0].localeCompare(b[0])).map(([hour, count]) => ({ hour, count }));

    return { items, totalCount: items.length, bySource, byHour, period };
}

export async function getHealthMetrics(period = 'hour') {
    await requireSuperAdmin();
    const now = new Date();
    const start = new Date(now);
    if (period === 'day') start.setDate(now.getDate() - 1);
    else if (period === 'week') start.setDate(now.getDate() - 7);
    else start.setHours(now.getHours() - 1);

    const [errorsResult, tracesResult] = await Promise.all([
        dataAccess.queryRecords(COLLECTION_KEYS.errors, {
            filters: { timestamp: { gte: start } },
            limit: 500,
            suppressAuth: true
        }),
        dataAccess.queryRecords(COLLECTION_KEYS.traces, {
            filters: { start_time: { gte: start.toISOString() } },
            limit: 500,
            suppressAuth: true
        })
    ]);

    const errors = (errorsResult.items || []).map(normalizeErrorRecord);
    const traces = (tracesResult.items || []).map(normalizeTraceRecord);
    const totalRequests = traces.length;
    const totalErrors = errors.length;
    const avgLatencyMs = traces.length ? Math.round(traces.reduce((sum, t) => sum + Number(t.duration || 0), 0) / traces.length) : 0;
    const errorRate = totalRequests > 0 ? Number(((totalErrors / totalRequests) * 100).toFixed(2)) : 0;

    const sourceAgg = {};
    traces.forEach((trace) => {
        const source = trace.source || 'system';
        if (!sourceAgg[source]) sourceAgg[source] = { source, requests: 0, errors: 0 };
        sourceAgg[source].requests += 1;
    });
    errors.forEach((error) => {
        const source = error.source || 'system';
        if (!sourceAgg[source]) sourceAgg[source] = { source, requests: 0, errors: 0 };
        sourceAgg[source].errors += 1;
    });

    const services = Object.values(sourceAgg).map((item) => ({
        ...item,
        errorRate: item.requests > 0 ? Number(((item.errors / item.requests) * 100).toFixed(2)) : 0,
        status: item.errors === 0 ? 'healthy' : item.errors <= 2 ? 'warning' : 'critical'
    }));

    return {
        period,
        status: errorRate > 10 ? 'critical' : errorRate > 2 ? 'warning' : 'healthy',
        totalRequests,
        totalErrors,
        errorRate,
        avgLatencyMs,
        services
    };
}

export async function getAIAnalytics(period = 'day') {
    await requireSuperAdmin();
    const now = new Date();
    const start = new Date(now);
    if (period === 'hour') start.setHours(now.getHours() - 1);
    else if (period === 'week') start.setDate(now.getDate() - 7);
    else if (period === 'month') start.setDate(now.getDate() - 30);
    else start.setDate(now.getDate() - 1);

    const result = await dataAccess.queryRecords(COLLECTION_KEYS.aiUsageLog, {
        filters: { timestamp: { gte: start } },
        limit: 2000,
        suppressAuth: true
    });

    const items = result.items || [];
    const providerMap = {};
    const functionMap = {};
    let totalTokens = 0;
    let totalLatency = 0;
    let totalRequests = 0;

    for (const item of items) {
        const provider = item.provider || 'unknown';
        const functionId = item.functionId || 'unknown';
        const tokens = Number(item.tokensUsed || 0);
        const latency = Number(item.latencyMs || 0);

        totalTokens += tokens;
        totalLatency += latency;
        totalRequests += 1;

        if (!providerMap[provider]) providerMap[provider] = { provider, requests: 0, tokens: 0, cost: 0, avgLatency: 0, latencyTotal: 0 };
        providerMap[provider].requests += 1;
        providerMap[provider].tokens += tokens;
        providerMap[provider].cost += tokens * 0.000002;
        providerMap[provider].latencyTotal += latency;

        if (!functionMap[functionId]) functionMap[functionId] = { functionId, requests: 0, tokens: 0, cost: 0, avgLatency: 0, latencyTotal: 0 };
        functionMap[functionId].requests += 1;
        functionMap[functionId].tokens += tokens;
        functionMap[functionId].cost += tokens * 0.000002;
        functionMap[functionId].latencyTotal += latency;
    }

    const byProvider = Object.values(providerMap).map((item) => ({
        provider: item.provider,
        requests: item.requests,
        tokens: item.tokens,
        cost: Number(item.cost.toFixed(4)),
        avgLatency: item.requests ? Math.round(item.latencyTotal / item.requests) : 0
    }));

    const byFunction = Object.values(functionMap).map((item) => ({
        functionId: item.functionId,
        requests: item.requests,
        tokens: item.tokens,
        cost: Number(item.cost.toFixed(4)),
        avgLatency: item.requests ? Math.round(item.latencyTotal / item.requests) : 0
    }));

    return {
        period,
        summary: {
            totalRequests,
            totalTokens,
            totalCost: Number((totalTokens * 0.000002).toFixed(4)),
            avgLatency: totalRequests ? Math.round(totalLatency / totalRequests) : 0
        },
        byProvider,
        byFunction
    };
}

export async function getMetrics(options = {}) {
    await requireSuperAdmin();
    const period = options.period || 'hour';
    const [health, anomalies] = await Promise.all([
        getHealthMetrics(period),
        getActiveAnomalies({ limit: 25 }).catch(() => [])
    ]);

    return {
        period,
        status: health.status,
        summary: {
            totalRequests: health.totalRequests,
            totalErrors: health.totalErrors,
            errorRate: health.errorRate,
            avgLatencyMs: health.avgLatencyMs,
            activeAnomalies: anomalies.length
        },
        services: health.services,
        anomalies
    };
}

export async function getAgentBehavior(options = {}) {
    await requireSuperAdmin();
    const period = options.period || 'day';
    const limit = Number(options.limit || 200);
    const now = new Date();
    const start = new Date(now);
    if (period === 'hour') start.setHours(now.getHours() - 1);
    else if (period === 'week') start.setDate(now.getDate() - 7);
    else if (period === 'month') start.setDate(now.getDate() - 30);
    else start.setDate(now.getDate() - 1);

    const [rawLogs, rawErrors, rawTraces, rawAiUsage] = await Promise.all([
        dataAccess.queryRecords(COLLECTION_KEYS.logs, {
            sort: [{ field: 'log_date', direction: 'desc' }],
            limit: Math.max(limit, 300),
            suppressAuth: true
        }),
        dataAccess.queryRecords(COLLECTION_KEYS.errors, {
            filters: { timestamp: { gte: start } },
            sort: [{ field: 'timestamp', direction: 'desc' }],
            limit: Math.max(limit, 300),
            suppressAuth: true
        }),
        dataAccess.queryRecords(COLLECTION_KEYS.traces, {
            filters: { start_time: { gte: start.toISOString() } },
            sort: [{ field: 'start_time', direction: 'desc' }],
            limit: Math.max(limit, 300),
            suppressAuth: true
        }),
        dataAccess.queryRecords(COLLECTION_KEYS.aiUsageLog, {
            filters: { timestamp: { gte: start } },
            limit: 1000,
            suppressAuth: true
        })
    ]);

    const logs = (rawLogs.items || [])
        .map(normalizeLogRecord)
        .filter((item) => new Date(item.timestamp || 0) >= start)
        .filter((item) => isAgentSource(item.source));
    const errors = (rawErrors.items || [])
        .map(normalizeErrorRecord)
        .filter((item) => isAgentSource(item.source));
    const traces = (rawTraces.items || [])
        .map(normalizeTraceRecord)
        .filter((item) => isAgentSource(item.source));
    const aiUsage = (rawAiUsage.items || []).filter((item) => isAgentUsage(item));

    const bySourceMap = {};
    [...logs, ...errors, ...traces].forEach((item) => {
        const source = item.source || 'unknown';
        if (!bySourceMap[source]) {
            bySourceMap[source] = { source, events: 0, errors: 0, runs: 0 };
        }
        bySourceMap[source].events += 1;
        if (String(item.level || '').toUpperCase() === 'ERROR' || String(item.level || '').toUpperCase() === 'CRITICAL') {
            bySourceMap[source].errors += 1;
        }
    });
    traces.forEach((trace) => {
        const source = trace.source || 'unknown';
        if (!bySourceMap[source]) {
            bySourceMap[source] = { source, events: 0, errors: 0, runs: 0 };
        }
        bySourceMap[source].runs += 1;
    });
    const bySource = Object.values(bySourceMap)
        .map((row) => ({
            ...row,
            errorRate: row.events > 0 ? Number(((row.errors / row.events) * 100).toFixed(2)) : 0
        }))
        .sort((a, b) => b.events - a.events);

    const runsByTrace = new Map();
    traces.forEach((trace) => {
        if (!trace.traceId) return;
        runsByTrace.set(trace.traceId, {
            traceId: trace.traceId,
            source: trace.source || 'system',
            status: trace.status || 'unknown',
            startedAt: trace.startTime || null,
            lastEventAt: trace.endTime || trace.startTime || null,
            durationMs: Number(trace.duration || 0),
            eventCount: 0,
            errorCount: 0,
            latestMessage: trace.name || 'trace',
            aiCalls: 0,
            aiTokens: 0
        });
    });

    [...logs, ...errors].forEach((evt) => {
        if (!evt.traceId) return;
        const row = runsByTrace.get(evt.traceId) || {
            traceId: evt.traceId,
            source: evt.source || 'system',
            status: 'in_progress',
            startedAt: evt.timestamp || null,
            lastEventAt: evt.timestamp || null,
            durationMs: Number(evt.duration || 0),
            eventCount: 0,
            errorCount: 0,
            latestMessage: evt.message || 'event',
            aiCalls: 0,
            aiTokens: 0
        };
        row.eventCount += 1;
        if (String(evt.level || '').toUpperCase() === 'ERROR' || String(evt.level || '').toUpperCase() === 'CRITICAL') {
            row.errorCount += 1;
            row.status = 'error';
        }
        if (!row.lastEventAt || new Date(evt.timestamp || 0) > new Date(row.lastEventAt || 0)) {
            row.lastEventAt = evt.timestamp || row.lastEventAt;
            row.latestMessage = evt.message || row.latestMessage;
        }
        runsByTrace.set(evt.traceId, row);
    });

    aiUsage.forEach((usage) => {
        const traceId = usage.traceId || usage.trace_id || null;
        if (!traceId) return;
        const row = runsByTrace.get(traceId);
        if (!row) return;
        row.aiCalls += 1;
        row.aiTokens += Number(usage.tokensUsed || 0);
        runsByTrace.set(traceId, row);
    });

    const recentRuns = Array.from(runsByTrace.values())
        .sort((a, b) => new Date(b.lastEventAt || 0).getTime() - new Date(a.lastEventAt || 0).getTime())
        .slice(0, Math.min(50, limit));

    const recentEvents = [...logs, ...errors]
        .sort((a, b) => new Date(b.timestamp || 0).getTime() - new Date(a.timestamp || 0).getTime())
        .slice(0, Math.min(100, limit))
        .map((event) => ({
            traceId: event.traceId || null,
            source: event.source || 'system',
            level: String(event.level || 'INFO').toUpperCase(),
            message: event.message || '',
            timestamp: event.timestamp || null
        }));

    const avgRunDurationMs = recentRuns.length > 0
        ? Math.round(recentRuns.reduce((sum, row) => sum + Number(row.durationMs || 0), 0) / recentRuns.length)
        : 0;
    const aiTokens = aiUsage.reduce((sum, row) => sum + Number(row.tokensUsed || 0), 0);

    return {
        period,
        summary: {
            totalRuns: recentRuns.length,
            totalEvents: logs.length + errors.length,
            totalErrors: errors.length,
            errorRate: (logs.length + errors.length) > 0 ? Number(((errors.length / (logs.length + errors.length)) * 100).toFixed(2)) : 0,
            avgRunDurationMs,
            aiCalls: aiUsage.length,
            aiTokens
        },
        bySource,
        recentRuns,
        recentEvents
    };
}

export async function getLogMetadata() {
    await requireSuperAdmin();
    return {
        levels: Object.keys(CONFIG.levels),
        sources: CONFIG.sources
    };
}

// ============================================
// ANOMALY DETECTION (Phase 2)
// ============================================

const DEFAULT_ANOMALY_RULES = [
    { name: 'API Error Spike', type: 'error_spike', metric: 'errors', condition: 'stddev_gt', threshold: 2, windowMinutes: 15, severity: 'critical', enabled: true, cooldownMinutes: 15, notifyEmail: true, notifyDashboard: true },
    { name: 'Latency Drift', type: 'latency_drift', metric: 'latency', condition: 'ratio_gt', threshold: 3, windowMinutes: 60, severity: 'warning', enabled: true, cooldownMinutes: 30, notifyEmail: false, notifyDashboard: true },
    { name: 'Cost Spike', type: 'cost_spike', metric: 'ai_cost', condition: 'ratio_gt', threshold: 1.5, windowMinutes: 60, severity: 'warning', enabled: true, cooldownMinutes: 30, notifyEmail: false, notifyDashboard: true },
    { name: 'Traffic Drop', type: 'traffic_drop', metric: 'requests', condition: 'ratio_lt', threshold: 0.5, windowMinutes: 30, severity: 'info', enabled: true, cooldownMinutes: 20, notifyEmail: false, notifyDashboard: true },
    { name: 'Job Failure', type: 'job_failure', metric: 'scheduler_runs', condition: 'missing_run', threshold: 1, windowMinutes: 10, severity: 'critical', enabled: true, cooldownMinutes: 10, notifyEmail: true, notifyDashboard: true }
];

export async function getAnomalyRules() {
    await requireSuperAdmin();
    const result = await dataAccess.queryRecords(COLLECTION_KEYS.anomalyRules, {
        sort: [{ field: 'name', direction: 'asc' }],
        limit: 200,
        suppressAuth: true
    });

    if ((result.items || []).length > 0) return result.items;

    return DEFAULT_ANOMALY_RULES;
}

export async function createAnomalyRule(rule) {
    await requireSuperAdmin();
    const payload = { ...rule, enabled: rule.enabled !== false, createdAt: new Date(), updatedAt: new Date() };
    return dataAccess.insertRecord(COLLECTION_KEYS.anomalyRules, payload, { suppressAuth: true });
}

export async function updateAnomalyRule(ruleId, updates) {
    await requireSuperAdmin();
    const existing = await dataAccess.getRecord(COLLECTION_KEYS.anomalyRules, ruleId, { suppressAuth: true });
    if (!existing) throw new Error('Rule not found');
    return dataAccess.updateRecord(COLLECTION_KEYS.anomalyRules, { ...existing, ...updates, _id: ruleId, updatedAt: new Date() }, { suppressAuth: true });
}

export async function deleteAnomalyRule(ruleId) {
    await requireSuperAdmin();
    return dataAccess.removeRecord(COLLECTION_KEYS.anomalyRules, ruleId, { suppressAuth: true });
}

export async function getActiveAnomalies(options = {}) {
    await requireSuperAdmin();
    const filters = { resolvedAt: { isEmpty: true } };
    if (options.severity) filters.severity = options.severity;
    if (options.type) filters.type = options.type;
    if (options.acknowledged !== undefined) filters.acknowledged = !!options.acknowledged;

    const result = await dataAccess.queryRecords(COLLECTION_KEYS.anomalyAlerts, {
        filters,
        sort: [{ field: 'detectedAt', direction: 'desc' }],
        limit: options.limit || 100,
        suppressAuth: true
    });
    return result.items || [];
}

export async function acknowledgeAnomaly(alertId) {
    await requireSuperAdmin();
    const existing = await dataAccess.getRecord(COLLECTION_KEYS.anomalyAlerts, alertId, { suppressAuth: true });
    if (!existing) throw new Error('Alert not found');
    return dataAccess.updateRecord(COLLECTION_KEYS.anomalyAlerts, {
        ...existing,
        _id: alertId,
        acknowledged: true,
        acknowledgedAt: new Date()
    }, { suppressAuth: true });
}

export async function resolveAnomaly(alertId, notes = '') {
    await requireSuperAdmin();
    const existing = await dataAccess.getRecord(COLLECTION_KEYS.anomalyAlerts, alertId, { suppressAuth: true });
    if (!existing) throw new Error('Alert not found');
    return dataAccess.updateRecord(COLLECTION_KEYS.anomalyAlerts, {
        ...existing,
        _id: alertId,
        resolvedAt: new Date(),
        resolutionNotes: notes,
        acknowledged: true
    }, { suppressAuth: true });
}

export async function getAnomalyHistory(period = 'week') {
    await requireSuperAdmin();
    const now = new Date();
    const start = new Date(now);
    if (period === 'day') start.setDate(now.getDate() - 1);
    else if (period === 'month') start.setDate(now.getDate() - 30);
    else start.setDate(now.getDate() - 7);

    const result = await dataAccess.queryRecords(COLLECTION_KEYS.anomalyAlerts, {
        filters: { detectedAt: { gte: start } },
        limit: 1000,
        suppressAuth: true
    });
    return result.items || [];
}

export async function calculateBaseline(metric, windowDays = 14) {
    await requireSuperAdmin();
    const now = new Date();
    const start = new Date(now);
    start.setDate(now.getDate() - windowDays);
    const sourceKey = metric === 'latency' ? COLLECTION_KEYS.traces : COLLECTION_KEYS.errors;
    const fieldName = metric === 'latency' ? 'duration' : '_id';

    const filters = metric === 'latency'
        ? { start_time: { gte: start.toISOString() } }
        : { timestamp: { gte: start } };
    const result = await dataAccess.queryRecords(sourceKey, {
        filters,
        limit: 2000,
        suppressAuth: true
    });
    const items = result.items || [];
    const values = items.map((item) => metric === 'latency' ? Number(item[fieldName] || 0) : 1);
    const mean = values.length ? values.reduce((sum, value) => sum + value, 0) / values.length : 0;
    const variance = values.length ? values.reduce((sum, value) => sum + ((value - mean) ** 2), 0) / values.length : 0;
    const stdDev = Math.sqrt(variance);

    return { metric, mean, stdDev, sampleCount: values.length, windowDays };
}

export async function updateBaselines() {
    const metrics = ['errors', 'latency'];
    const now = new Date();
    const updates = [];
    for (const metric of metrics) {
        const baseline = await calculateBaseline(metric, 14);
        const payload = {
            metric,
            source: 'system',
            hourOfDay: now.getHours(),
            dayOfWeek: now.getDay(),
            mean: Number(baseline.mean.toFixed(4)),
            stdDev: Number(baseline.stdDev.toFixed(4)),
            sampleCount: baseline.sampleCount,
            lastUpdated: now
        };
        updates.push(await dataAccess.insertRecord(COLLECTION_KEYS.baselineMetrics, payload, { suppressAuth: true }));
    }
    return { success: true, updated: updates.length };
}

export async function runAnomalyDetection() {
    const rules = await getAnomalyRules();
    const activeRules = (rules || []).filter((rule) => rule.enabled !== false);
    const now = new Date();
    const generated = [];

    for (const rule of activeRules) {
        const windowStart = new Date(now);
        windowStart.setMinutes(now.getMinutes() - Number(rule.windowMinutes || 15));

        const recentAlerts = await dataAccess.queryRecords(COLLECTION_KEYS.anomalyAlerts, {
            filters: {
                type: rule.type,
                detectedAt: { gte: new Date(now.getTime() - Number(rule.cooldownMinutes || 15) * 60 * 1000) }
            },
            limit: 1,
            suppressAuth: true
        });
        if ((recentAlerts.items || []).length > 0) continue;

        let triggered = false;
        let actualValue = 0;
        let expectedValue = 0;

        if (rule.type === 'error_spike') {
            const baseline = await calculateBaseline('errors', 14);
            const errors = await dataAccess.queryRecords(COLLECTION_KEYS.errors, {
                filters: { timestamp: { gte: windowStart } },
                limit: 1000,
                suppressAuth: true
            });
            actualValue = (errors.items || []).length;
            expectedValue = baseline.mean + (Number(rule.threshold || 2) * baseline.stdDev);
            triggered = actualValue > expectedValue && expectedValue > 0;
        } else if (rule.type === 'latency_drift') {
            const baseline = await calculateBaseline('latency', 14);
            const traces = await dataAccess.queryRecords(COLLECTION_KEYS.traces, {
                filters: { start_time: { gte: windowStart.toISOString() } },
                limit: 1000,
                suppressAuth: true
            });
            const latencies = (traces.items || []).map((item) => Number(item.duration || 0)).filter((value) => value > 0);
            actualValue = latencies.length ? (latencies.reduce((sum, value) => sum + value, 0) / latencies.length) : 0;
            expectedValue = baseline.mean * Number(rule.threshold || 3);
            triggered = actualValue > expectedValue && expectedValue > 0;
        } else if (rule.type === 'traffic_drop') {
            const traces = await dataAccess.queryRecords(COLLECTION_KEYS.traces, {
                filters: { start_time: { gte: windowStart.toISOString() } },
                limit: 1000,
                suppressAuth: true
            });
            const priorStart = new Date(windowStart);
            priorStart.setMinutes(priorStart.getMinutes() - Number(rule.windowMinutes || 30));
            const prior = await dataAccess.queryRecords(COLLECTION_KEYS.traces, {
                filters: { start_time: { gte: priorStart.toISOString(), lt: windowStart.toISOString() } },
                limit: 1000,
                suppressAuth: true
            });
            actualValue = (traces.items || []).length;
            const priorCount = (prior.items || []).length;
            expectedValue = priorCount * Number(rule.threshold || 0.5);
            triggered = priorCount > 0 && actualValue < expectedValue;
        } else if (rule.type === 'cost_spike') {
            const usage = await dataAccess.queryRecords(COLLECTION_KEYS.aiUsageLog, {
                filters: { timestamp: { gte: windowStart } },
                limit: 1000,
                suppressAuth: true
            });
            actualValue = (usage.items || []).reduce((sum, item) => sum + Number(item.tokensUsed || 0), 0);
            const baseline = await calculateBaseline('errors', 14);
            expectedValue = Math.max(1, baseline.sampleCount) * Number(rule.threshold || 1.5);
            triggered = actualValue > expectedValue;
        } else if (rule.type === 'job_failure') {
            const alerts = await dataAccess.queryRecords(COLLECTION_KEYS.alerts, {
                filters: { status: 'error', timestamp: { gte: windowStart } },
                limit: 100,
                suppressAuth: true
            });
            actualValue = (alerts.items || []).length;
            expectedValue = 0;
            triggered = actualValue > 0;
        }

        if (triggered) {
            const deviation = expectedValue > 0 ? Number(((actualValue - expectedValue) / expectedValue).toFixed(4)) : 1;
            const alert = await dataAccess.insertRecord(COLLECTION_KEYS.anomalyAlerts, {
                type: rule.type,
                severity: rule.severity || 'warning',
                source: rule.source || 'system',
                metric: rule.metric,
                expectedValue,
                actualValue,
                deviation,
                message: `${rule.name} detected`,
                detectedAt: now,
                acknowledged: false,
                autoResolved: false
            }, { suppressAuth: true });
            generated.push(alert);
        }
    }

    await autoResolveAnomalies();
    await escalateCriticalAnomalies();
    return { success: true, triggered: generated.length };
}

async function autoResolveAnomalies() {
    const active = await dataAccess.queryRecords(COLLECTION_KEYS.anomalyAlerts, {
        filters: { resolvedAt: { isEmpty: true } },
        limit: 200,
        suppressAuth: true
    });
    const now = new Date();
    for (const alert of (active.items || [])) {
        if (Number(alert.deviation || 0) < 0.05) {
            await dataAccess.updateRecord(COLLECTION_KEYS.anomalyAlerts, {
                ...alert,
                _id: alert._id,
                autoResolved: true,
                resolvedAt: now
            }, { suppressAuth: true });
        }
    }
}

async function escalateCriticalAnomalies() {
    const threshold = new Date(Date.now() - 15 * 60 * 1000);
    const result = await dataAccess.queryRecords(COLLECTION_KEYS.anomalyAlerts, {
        filters: {
            severity: 'critical',
            acknowledged: false,
            detectedAt: { lte: threshold },
            resolvedAt: { isEmpty: true }
        },
        limit: 100,
        suppressAuth: true
    });

    for (const alert of (result.items || [])) {
        await dataAccess.updateRecord(COLLECTION_KEYS.anomalyAlerts, {
            ...alert,
            _id: alert._id,
            escalated: true,
            escalatedAt: new Date()
        }, { suppressAuth: true });
    }
}

// ============================================
// HELPER FUNCTIONS
// ============================================

function generateTraceId() {
    return 'tr_' + Date.now().toString(36) + Math.random().toString(36).substr(2, 9);
}

function generateSpanId() {
    return 'sp_' + Math.random().toString(36).substr(2, 12);
}

function safeJson(value) {
    try {
        return JSON.stringify(value ?? {});
    } catch (error) {
        return JSON.stringify({ serializeError: true });
    }
}

function sanitizeForStorage(value) {
    if (value === null || value === undefined) return {};
    const serialized = typeof value === 'string' ? value : safeJson(value);
    if (serialized.length > CONFIG.logSampling.maxDetailsLength) {
        return serialized.slice(0, CONFIG.logSampling.maxDetailsLength);
    }
    return typeof value === 'string' ? value : value;
}

function toTagString(tags) {
    return (tags || [])
        .filter(Boolean)
        .map((tag) => String(tag).trim())
        .filter(Boolean)
        .join(', ');
}

function parseMaybeJson(value) {
    if (typeof value !== 'string') return value;
    try {
        return JSON.parse(value);
    } catch (error) {
        return value;
    }
}

function normalizeLogRecord(item) {
    return {
        _id: item._id || item.id || item.log_id || generateSpanId(),
        level: String(item.level || 'INFO').toUpperCase(),
        source: item.source || item.service || 'system',
        message: item.message || '',
        timestamp: item.timestamp || item.log_date || null,
        duration: item.duration || null,
        traceId: item.trace_id || item.traceId || null,
        tags: item.tags ? String(item.tags).split(',').map((tag) => tag.trim()).filter(Boolean) : [],
        details: parseMaybeJson(item.details || item.metadata || null)
    };
}

function normalizeErrorRecord(item) {
    return {
        ...item,
        _id: item._id || item.id || item.wix_id || generateSpanId(),
        level: String(item.level || 'ERROR').toUpperCase(),
        source: item.source || item.service || 'system',
        message: item.message || '',
        timestamp: item.timestamp || item.log_date || null,
        traceId: item.trace_id || item.traceId || null,
        duration: item.duration || null,
        details: parseMaybeJson(item.details || null)
    };
}

function normalizeTraceRecord(item) {
    const metadata = parseMaybeJson(item.metadata || {}) || {};
    return {
        ...item,
        _id: item._id || item.id || item.wix_id || null,
        traceId: item.traceId || item.trace_id || null,
        name: item.name || 'operation',
        source: item.source || metadata.source || 'system',
        startTime: item.startTime || item.start_time || null,
        endTime: item.endTime || item.end_time || null,
        status: item.status || 'unknown',
        spans: parseMaybeJson(item.spans || []) || [],
        metadata,
        tags: item.tags ? String(item.tags).split(',').map((tag) => tag.trim()).filter(Boolean) : [],
        duration: Number(item.duration || 0),
        summary: parseMaybeJson(item.summary || {}) || {}
    };
}

function buildTraceTimeline(trace, logs) {
    const spanEvents = (trace.spans || []).map((span) => ({
        timestamp: span.endTime || span.startTime || trace.startTime,
        spanId: span.spanId || null,
        parentSpanId: span.parentSpanId || null,
        message: span.name || 'span',
        duration: span.duration || null,
        level: span.status === 'error' ? 'ERROR' : 'INFO'
    }));

    return [...spanEvents, ...(logs || [])]
        .map((entry) => ({
            timestamp: entry.timestamp || entry.startTime || trace.startTime,
            spanId: entry.spanId || entry.span_id || null,
            parentSpanId: entry.parentSpanId || entry.parent_span_id || null,
            message: entry.message || 'event',
            duration: entry.duration || null,
            level: String(entry.level || 'INFO').toUpperCase()
        }))
        .sort((a, b) => new Date(a.timestamp || 0).getTime() - new Date(b.timestamp || 0).getTime());
}

function isAgentSource(source) {
    const normalized = String(source || '').toLowerCase();
    return AGENT_SOURCES.includes(normalized) || normalized.includes('agent');
}

function isAgentUsage(item) {
    const functionId = String(item.functionId || '').toLowerCase();
    return functionId.includes('agent') || functionId.includes('orchestration') || functionId.includes('b2b');
}
