{
  "track_id": "ai_intelligence_layer_20260219",
  "title": "AI Intelligence Layer — Semantic Search, Agent Streaming & B2B Multi-Agent",
  "type": "feature",
  "status": "planned",
  "created": "2026-02-19",
  "updated": "2026-02-19",
  "priority": "high",
  "estimated_phases": 4,
  "completed_phases": 0,
  "completion_percentage": 0,
  "dependencies": [
    "full_agentic_buildout_20260218",
    "reverse_matching_20251225",
    "b2b_business_development_suite_20260128",
    "agentService.jsw",
    "aiRouterService.jsw",
    "driverScoring.js",
    "b2bResearchAgentService.jsw"
  ],
  "owners": [],
  "tags": [
    "ai",
    "semantic-search",
    "rag",
    "vector-search",
    "langchain",
    "llamaindex",
    "streaming",
    "multi-agent",
    "b2b",
    "langsmith",
    "microservice"
  ],
  "business_impact": {
    "type": "matching_quality_and_agent_capability",
    "description": "Close the three AI capability gaps identified in the LangChain framework analysis: semantic driver/carrier matching (highest value), agent response streaming (UX), and parallel B2B research agents (intelligence product). All require a thin external microservice since Wix Velo cannot run LangChain/LlamaIndex natively."
  },
  "architecture_constraint": "Wix Velo is not a Node.js runtime — no npm packages with Node.js internals can run inside .jsw files. All framework-dependent code must live in an external microservice (Railway/Render/Vercel) called by Velo via fetch(). Velo remains the orchestration layer; the microservice owns embeddings, vector queries, streaming, and multi-agent coordination.",
  "phases": {
    "1": {
      "name": "Semantic Search Microservice",
      "priority": "yes",
      "framework": "LlamaIndex + Pinecone (or pgvector)",
      "description": "Vector embeddings for drivers and carriers; semantic similarity search replacing pure filter-based matching."
    },
    "2": {
      "name": "Agent Observability — LangSmith Tracing",
      "priority": "yes",
      "framework": "LangSmith",
      "description": "Trace every agent turn (tool calls, latency, failures) via LangSmith. Gives product-level visibility into agent quality."
    },
    "3": {
      "name": "Streaming Agent Responses",
      "priority": "low",
      "framework": "Vercel AI SDK or raw SSE",
      "description": "Stream agent token output to HTML components via chunked postMessage instead of buffering full response."
    },
    "4": {
      "name": "B2B Parallel Research Agents",
      "priority": "maybe",
      "framework": "CrewAI or AutoGen",
      "description": "Replace sequential b2bResearchAgentService with parallel sub-agents (FMCSA, social, news, LinkedIn) coordinated by a supervisor agent."
    }
  },
  "metrics": {
    "target_match_relevance_improvement": ">= 20% click-through lift on semantic vs filter results",
    "target_agent_trace_coverage": "100% of agent turns captured in LangSmith",
    "target_streaming_ttft": "< 800ms time-to-first-token on agent responses",
    "target_b2b_research_latency": "< 15s for full company intelligence report (parallel vs 45s+ sequential)"
  }
}
